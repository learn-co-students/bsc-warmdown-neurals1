{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06234542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:42.547428Z",
     "start_time": "2021-08-03T21:30:41.190826Z"
    },
    "index": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from helpers import plot_decision_boundary\n",
    "np.random.seed(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c838f61",
   "metadata": {
    "index": 1
   },
   "source": [
    "In the cell below we create `X` and `Y` variables and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e080ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:42.762022Z",
     "start_time": "2021-08-03T21:30:42.550425Z"
    },
    "index": 2
   },
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "X, Y = noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa530a",
   "metadata": {
    "index": 3
   },
   "source": [
    "The dimensions of our data are extremely important when building neural networks. \n",
    "\n",
    "In the cell below, output the dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9762a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:42.771532Z",
     "start_time": "2021-08-03T21:30:42.767462Z"
    },
    "index": 4
   },
   "outputs": [],
   "source": [
    "n_features = None\n",
    "n_observations = None\n",
    "\n",
    "print(f'There are {n_features} features in these data.')\n",
    "print(f'There are {n_observations} observations in these data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbaca87",
   "metadata": {
    "index": 6
   },
   "source": [
    "Great. Let's see how LogisticRegression does at predicting these data.\n",
    "\n",
    "In the cell below, create a logistic regression model called `log_reg` and fit it to the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:42.785889Z",
     "start_time": "2021-08-03T21:30:42.782467Z"
    },
    "index": 7
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a968a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:42.798009Z",
     "start_time": "2021-08-03T21:30:42.788259Z"
    },
    "index": 8
   },
   "outputs": [],
   "source": [
    "#__SOUTION__\n",
    "log_reg = LogisticRegression();\n",
    "log_reg.fit(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69666bc4",
   "metadata": {
    "index": 9
   },
   "source": [
    "Now let's plot how well the model seperated the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55d5bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:43.134727Z",
     "start_time": "2021-08-03T21:30:42.801806Z"
    },
    "index": 10
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: log_reg.predict(x), X, Y)\n",
    "plt.title(\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d2e06",
   "metadata": {
    "index": 11
   },
   "source": [
    "Not particularly great. This is a problem that Logistic Regression is ill suited for. \n",
    "\n",
    "**Let's see if a Neural Network can do better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0982cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:46.777304Z",
     "start_time": "2021-08-03T21:30:43.139058Z"
    },
    "index": 12
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d256f1",
   "metadata": {
    "index": 13
   },
   "source": [
    "**Create a simple keras model**\n",
    "\n",
    "Below, we create a model with a single hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa7725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:30:46.783957Z",
     "start_time": "2021-08-03T21:30:46.779158Z"
    },
    "index": 14
   },
   "outputs": [],
   "source": [
    "def tensor_flow_model(n_units=5):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "\n",
    "    #====== INPUT LAYER =======\n",
    "        # The shape of this layer \n",
    "        # should = the number of columns\n",
    "        # in your datA\n",
    "    input_layer = Input(shape=(2,))\n",
    "    \n",
    "    \n",
    "    # ====== HIDDEN LAYER ======\n",
    "        # We will use a sigmoid activation function\n",
    "        # because we are prediction values of zero and one.\n",
    "        # The number of units usually requires experimentation\n",
    "        # So it is set as an adjustable parameter.\n",
    "    hidden_layer = Dense(units=n_units, \n",
    "                         activation='sigmoid')\n",
    "    \n",
    "    \n",
    "    # ===== OUTPUT LAYER =======\n",
    "        # We want to output a single number for each oberservation\n",
    "        # So our output units (or output shape) will be 1. \n",
    "        # This is a classification problem for targets 0 and 1\n",
    "        # so sigmoid makes a good activation function for our\n",
    "        # output layer.\n",
    "    output_layer = Dense(units=1, activation='sigmoid')\n",
    "    \n",
    "\n",
    "    # Create neural network model\n",
    "    model.add(input_layer)\n",
    "    model.add(hidden_layer)\n",
    "    model.add(output_layer)\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0a80c",
   "metadata": {
    "index": 15
   },
   "source": [
    "Now we set the number of units to 5 and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5e35f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:28.773921Z",
     "start_time": "2021-08-03T21:30:46.786922Z"
    },
    "index": 16
   },
   "outputs": [],
   "source": [
    "# Create mode object\n",
    "n_units=5\n",
    "tflow_model = tensor_flow_model(n_units=n_units)\n",
    "\n",
    "# fit model\n",
    "tflow_model.fit(X, Y, \n",
    "                epochs=10000, # Number of iterations \n",
    "                verbose=0, # Turn off print outs of each epoch\n",
    "                batch_size=1500,  # How many times should we update the weights for each epoch\n",
    "                shuffle=False) # Keep the rows from being shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775bb70",
   "metadata": {
    "index": 17
   },
   "source": [
    "Let's plot the neural networks performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0ba3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.322916Z",
     "start_time": "2021-08-03T21:31:28.776158Z"
    },
    "index": 18
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: tflow_model.predict_classes(x), X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe3ae2",
   "metadata": {
    "index": 19
   },
   "source": [
    "Nice that is performing with near perfect predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c27b8b",
   "metadata": {
    "index": 20
   },
   "source": [
    "###### So what is happening here?\n",
    "\n",
    "Let's visualize what this model looks like:\n",
    "\n",
    "![](neural_net.jpg)\n",
    "\n",
    "\n",
    "### Neural Network Notation:\n",
    "\n",
    "A subscript represents the *node* (unit) of the neural network\n",
    "* $x_1$ represents the first node of the input layer.\n",
    "\n",
    "The letter $a$ represents an output or a layer or node. Whether it represents the output of a layer or node is indicated by subscripts and superscripts.\n",
    "\n",
    "A superscript surrounded by brackets represents a specific layer\n",
    "* $a^{[0]}$ represents the output of the input layer\n",
    "* $a^{[1]}$ represents the output of the first hidden layer\n",
    "\n",
    "A superscript surrounded by parentheses represents a specific observation in the data\n",
    "* $a^{[1](0)}$ represents the output of the first hidden layer for the first observation.\n",
    "    * We do not use this notation in the above image, but you will encounter this notation in the wild. \n",
    "\n",
    "$Z$ represents the output of the linear equation that occures inside the nodes of a layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4328607",
   "metadata": {
    "index": 21
   },
   "source": [
    "Let's break down the steps of a the neural network. \n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "During intialization we:\n",
    "1. Set the number of layers and the number of nodes for each layer\n",
    "2. For each layer we initialize a `W` weight matrix that represents the weights for each note. This matrix has the shape:\n",
    "\n",
    "    * ```(number of units in next layer, number of units in the previous layer)```\n",
    "3. For each layer we initialize a `b` weight matrix that represents the bias/intercept terms for each node\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86641f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.328107Z",
     "start_time": "2021-08-03T21:31:32.324119Z"
    },
    "index": 22
   },
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "\n",
    "def initialize(X, Y, hidden_units=5):\n",
    "    # Initialize the shape of input \n",
    "    # and output layers\n",
    "    number_of_examples = X.shape[0]\n",
    "    output_layer_size = Y.shape[0]\n",
    "\n",
    "    \n",
    "    W1 = np.random.randn(hidden_units, number_of_examples) * 0.01\n",
    "    b1 = np.zeros(shape=(hidden_units, 1))\n",
    "    W2 = np.random.randn(output_layer_size, hidden_units) * 0.01\n",
    "    b2 = np.zeros(shape=(output_layer_size, 1))\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc70926",
   "metadata": {
    "index": 23
   },
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "During forward propagation we:\n",
    "\n",
    "1. Multiply the weights of the first layer with input data and add the bias for first the first layer\n",
    "2. Pass the output of the step above into a sigmoid function\n",
    "3. Multiple the weights of the second layer with the output of the last step and add the bias for the second layer\n",
    "4. Pass the results of the above step into the sigmoid function\n",
    "5. Store the results of each step in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37780c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.334066Z",
     "start_time": "2021-08-03T21:31:32.329671Z"
    },
    "index": 24
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    # Collect parameters from \n",
    "    # the inputed dictionary\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    # Sigmoid helper function\n",
    "    sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "\n",
    "    # Multiply the weights of the first layer with input data \n",
    "    # and add the bias for first the first layer\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    # Pass the output of the step above into a sigmoid function\n",
    "    A1 = np.tanh(Z1)\n",
    "    # Multiple the weights of the second layer with the output \n",
    "    # of the last step and add the bias for the second layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    # Pass the results of the above step into the sigmoid function\n",
    "    prediction = sigmoid(Z2)\n",
    "\n",
    "    assert(prediction.shape == (1, X.shape[1]))\n",
    "    \n",
    "    # Store the results of each step in a dictionary\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"prediction\": prediction}\n",
    "    \n",
    "    # Return the output\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7ca76",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Caculate Error\n",
    "\n",
    "In this step we calculate the error of a prediction produced via forward propogation. We caculate the error of our model using the below cost function:\n",
    "\n",
    "![](cost-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15174e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:19:55.526683Z",
     "start_time": "2021-08-03T20:19:55.522940Z"
    },
    "index": 26
   },
   "source": [
    "$h_\\theta$ = The model\n",
    "\n",
    "$x^{(i)}$ = An individual observation in `X`\n",
    "\n",
    "$h_\\theta(x^{(i)})$ = The model prediction for the\n",
    "\n",
    "$y^{(i)}$ = The true class label for $x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511c929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.338811Z",
     "start_time": "2021-08-03T21:31:32.335965Z"
    },
    "index": 27
   },
   "outputs": [],
   "source": [
    "def calculate_error(prediction, Y):\n",
    "     \n",
    "    # Calculate the number of samples\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Calcuate the inner equation\n",
    "    logprobs = np.multiply(np.log(prediction), Y) + np.multiply((1 - Y), np.log(1 - prediction))\n",
    "    # Sum the results and divide by the number of samples\n",
    "    error = - np.sum(logprobs) / m\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523fcd7",
   "metadata": {
    "index": 28
   },
   "source": [
    "#### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324266b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.345396Z",
     "start_time": "2021-08-03T21:31:32.340139Z"
    },
    "index": 29
   },
   "outputs": [],
   "source": [
    "def backward_propagation(params, cache, X, Y, learning_rate = .001):\n",
    "\n",
    "    # Calcuate the total number of observations\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Collect the weights for each layer\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    b1 = params['b1']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    # Collect the output of each layer\n",
    "    A1 = cache['A1']\n",
    "    prediction = cache['prediction']\n",
    "    \n",
    "    # Caculate the derivatives with respect to the linear\n",
    "    # equation for layer 2\n",
    "    dZ2= prediction - Y\n",
    "    # Calculate the derivative with respect to the weights \n",
    "    # for layer 2\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    # Caculate the derivative with respect to the intercept  \n",
    "    # for layer 2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # Caculate the derivatives with respect to the linear\n",
    "    # equation for layer 1\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    # Calculate the derivative with respect to the weights \n",
    "    # for layer 1\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    # Caculate the derivative with respect to the intercept  \n",
    "    # for layer 1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update the weights and bias terms\n",
    "    # for each layer\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    \n",
    "    # Store the updated results in a dictionary\n",
    "    updated_params = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "\n",
    "    # Return the results\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f41369",
   "metadata": {
    "index": 30
   },
   "source": [
    "#### And that's it! We would then run the procress of forward propagation, calculate error, and backward propagation in a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b7b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:32.351163Z",
     "start_time": "2021-08-03T21:31:32.346975Z"
    },
    "index": 31
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def neural_network(X, Y, hidden_units = 5, max_iter=10000, learning_rate=1.2):\n",
    "    # Initialize mode parameters\n",
    "    params = initialize(X, Y)\n",
    "    # Run forward propogation and backwards propagation\n",
    "    # max_iter number of times\n",
    "    for i in range(max_iter):\n",
    "        # Generate the outputs for each layer\n",
    "        forward_prop = forward_propagation(X, params)\n",
    "        # Pull out the prediction vaue\n",
    "        prediction = forward_prop['prediction']\n",
    "        # Update the weights for the model using gradient descent\n",
    "        params = backward_propagation(params, forward_prop, X, Y, learning_rate=learning_rate)\n",
    "        # Calculate the error for the mode\n",
    "        cost = calculate_error(prediction, Y)\n",
    "        # Print the cost after every 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        \n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d566586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T21:31:36.106239Z",
     "start_time": "2021-08-03T21:31:32.352889Z"
    },
    "index": 32
   },
   "outputs": [],
   "source": [
    "params = neural_network(X.T, Y.reshape(1, X.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
